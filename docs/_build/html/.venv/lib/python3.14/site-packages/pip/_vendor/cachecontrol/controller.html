

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>SPDX-FileCopyrightText: 2015 Eric Larson &mdash; Retrogue 0.0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../_static/custom.css?v=a6a68382" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../_static/fonts.css?v=5583d106" />

  
      <script src="../../../../../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../../../../../_static/documentation_options.js?v=d45e8c67"></script>
      <script src="../../../../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../../../index.html" class="icon icon-home">
            Retrogue
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../modules.html">src</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../../index.html">Retrogue</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">SPDX-FileCopyrightText: 2015 Eric Larson</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../../_sources/.venv/lib/python3.14/site-packages/pip/_vendor/cachecontrol/controller.py.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="spdx-filecopyrighttext-2015-eric-larson">
<h1>SPDX-FileCopyrightText: 2015 Eric Larson<a class="headerlink" href="#spdx-filecopyrighttext-2015-eric-larson" title="Link to this heading"></a></h1>
</section>
<section id="id1">
<h1><a class="headerlink" href="#id1" title="Link to this heading"></a></h1>
</section>
<section id="spdx-license-identifier-apache-2-0">
<h1>SPDX-License-Identifier: Apache-2.0<a class="headerlink" href="#spdx-license-identifier-apache-2-0" title="Link to this heading"></a></h1>
<p>“””
The httplib2 algorithms ported for use with requests.
“””</p>
<p>from <strong>future</strong> import annotations</p>
<p>import calendar
import logging
import re
import time
import weakref
from email.utils import parsedate_tz
from typing import TYPE_CHECKING, Collection, Mapping</p>
<p>from pip._vendor.requests.structures import CaseInsensitiveDict</p>
<p>from pip._vendor.cachecontrol.cache import DictCache, SeparateBodyBaseCache
from pip._vendor.cachecontrol.serialize import Serializer</p>
<p>if TYPE_CHECKING:
from typing import Literal</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>from pip._vendor.requests import PreparedRequest
from pip._vendor.urllib3 import HTTPResponse

from pip._vendor.cachecontrol.cache import BaseCache
</pre></div>
</div>
<p>logger = logging.getLogger(<strong>name</strong>)</p>
<p>URI = re.compile(r”^((<a href="#id7"><span class="problematic" id="id2">[^:/?#]</span></a>+):)?(//(<a href="#id8"><span class="problematic" id="id3">[^/?#]</span></a><em>))?(<a href="#id9"><span class="problematic" id="id4">[^?#]</span></a></em>)(?(<a href="#id10"><span class="problematic" id="id5">[^#]</span></a><em>))?(#(.</em>))?”)</p>
<p>PERMANENT_REDIRECT_STATUSES = (301, 308)</p>
<p>def parse_uri(uri: str) -&gt; tuple[str, str, str, str, str]:
“””Parses a URI using the regex given in Appendix B of RFC 3986.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>(scheme, authority, path, query, fragment) = parse_uri(uri)
&quot;&quot;&quot;
match = URI.match(uri)
assert match is not None
groups = match.groups()
return (groups[1], groups[3], groups[4], groups[6], groups[8])
</pre></div>
</div>
<p>class CacheController:
“””An interface to see if request should cached or not.”””</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>def __init__(
    self,
    cache: BaseCache | None = None,
    cache_etags: bool = True,
    serializer: Serializer | None = None,
    status_codes: Collection[int] | None = None,
):
    self.cache = DictCache() if cache is None else cache
    self.cache_etags = cache_etags
    self.serializer = serializer or Serializer()
    self.cacheable_status_codes = status_codes or (200, 203, 300, 301, 308)

@classmethod
def _urlnorm(cls, uri: str) -&gt; str:
    &quot;&quot;&quot;Normalize the URL to create a safe key for the cache&quot;&quot;&quot;
    (scheme, authority, path, query, fragment) = parse_uri(uri)
    if not scheme or not authority:
        raise Exception(&quot;Only absolute URIs are allowed. uri = %s&quot; % uri)

    scheme = scheme.lower()
    authority = authority.lower()

    if not path:
        path = &quot;/&quot;

    # Could do syntax based normalization of the URI before
    # computing the digest. See Section 6.2.2 of Std 66.
    request_uri = query and &quot;?&quot;.join([path, query]) or path
    defrag_uri = scheme + &quot;://&quot; + authority + request_uri

    return defrag_uri

@classmethod
def cache_url(cls, uri: str) -&gt; str:
    return cls._urlnorm(uri)

def parse_cache_control(self, headers: Mapping[str, str]) -&gt; dict[str, int | None]:
    known_directives = {
        # https://tools.ietf.org/html/rfc7234#section-5.2
        &quot;max-age&quot;: (int, True),
        &quot;max-stale&quot;: (int, False),
        &quot;min-fresh&quot;: (int, True),
        &quot;no-cache&quot;: (None, False),
        &quot;no-store&quot;: (None, False),
        &quot;no-transform&quot;: (None, False),
        &quot;only-if-cached&quot;: (None, False),
        &quot;must-revalidate&quot;: (None, False),
        &quot;public&quot;: (None, False),
        &quot;private&quot;: (None, False),
        &quot;proxy-revalidate&quot;: (None, False),
        &quot;s-maxage&quot;: (int, True),
    }

    cc_headers = headers.get(&quot;cache-control&quot;, headers.get(&quot;Cache-Control&quot;, &quot;&quot;))

    retval: dict[str, int | None] = {}

    for cc_directive in cc_headers.split(&quot;,&quot;):
        if not cc_directive.strip():
            continue

        parts = cc_directive.split(&quot;=&quot;, 1)
        directive = parts[0].strip()

        try:
            typ, required = known_directives[directive]
        except KeyError:
            logger.debug(&quot;Ignoring unknown cache-control directive: %s&quot;, directive)
            continue

        if not typ or not required:
            retval[directive] = None
        if typ:
            try:
                retval[directive] = typ(parts[1].strip())
            except IndexError:
                if required:
                    logger.debug(
                        &quot;Missing value for cache-control &quot; &quot;directive: %s&quot;,
                        directive,
                    )
            except ValueError:
                logger.debug(
                    &quot;Invalid value for cache-control directive &quot; &quot;%s, must be %s&quot;,
                    directive,
                    typ.__name__,
                )

    return retval

def _load_from_cache(self, request: PreparedRequest) -&gt; HTTPResponse | None:
    &quot;&quot;&quot;
    Load a cached response, or return None if it&#39;s not available.
    &quot;&quot;&quot;
    # We do not support caching of partial content: so if the request contains a
    # Range header then we don&#39;t want to load anything from the cache.
    if &quot;Range&quot; in request.headers:
        return None

    cache_url = request.url
    assert cache_url is not None
    cache_data = self.cache.get(cache_url)
    if cache_data is None:
        logger.debug(&quot;No cache entry available&quot;)
        return None

    if isinstance(self.cache, SeparateBodyBaseCache):
        body_file = self.cache.get_body(cache_url)
    else:
        body_file = None

    result = self.serializer.loads(request, cache_data, body_file)
    if result is None:
        logger.warning(&quot;Cache entry deserialization failed, entry ignored&quot;)
    return result

def cached_request(self, request: PreparedRequest) -&gt; HTTPResponse | Literal[False]:
    &quot;&quot;&quot;
    Return a cached response if it exists in the cache, otherwise
    return False.
    &quot;&quot;&quot;
    assert request.url is not None
    cache_url = self.cache_url(request.url)
    logger.debug(&#39;Looking up &quot;%s&quot; in the cache&#39;, cache_url)
    cc = self.parse_cache_control(request.headers)

    # Bail out if the request insists on fresh data
    if &quot;no-cache&quot; in cc:
        logger.debug(&#39;Request header has &quot;no-cache&quot;, cache bypassed&#39;)
        return False

    if &quot;max-age&quot; in cc and cc[&quot;max-age&quot;] == 0:
        logger.debug(&#39;Request header has &quot;max_age&quot; as 0, cache bypassed&#39;)
        return False

    # Check whether we can load the response from the cache:
    resp = self._load_from_cache(request)
    if not resp:
        return False

    # If we have a cached permanent redirect, return it immediately. We
    # don&#39;t need to test our response for other headers b/c it is
    # intrinsically &quot;cacheable&quot; as it is Permanent.
    #
    # See:
    #   https://tools.ietf.org/html/rfc7231#section-6.4.2
    #
    # Client can try to refresh the value by repeating the request
    # with cache busting headers as usual (ie no-cache).
    if int(resp.status) in PERMANENT_REDIRECT_STATUSES:
        msg = (
            &quot;Returning cached permanent redirect response &quot;
            &quot;(ignoring date and etag information)&quot;
        )
        logger.debug(msg)
        return resp

    headers: CaseInsensitiveDict[str] = CaseInsensitiveDict(resp.headers)
    if not headers or &quot;date&quot; not in headers:
        if &quot;etag&quot; not in headers:
            # Without date or etag, the cached response can never be used
            # and should be deleted.
            logger.debug(&quot;Purging cached response: no date or etag&quot;)
            self.cache.delete(cache_url)
        logger.debug(&quot;Ignoring cached response: no date&quot;)
        return False

    now = time.time()
    time_tuple = parsedate_tz(headers[&quot;date&quot;])
    assert time_tuple is not None
    date = calendar.timegm(time_tuple[:6])
    current_age = max(0, now - date)
    logger.debug(&quot;Current age based on date: %i&quot;, current_age)

    # TODO: There is an assumption that the result will be a
    #       urllib3 response object. This may not be best since we
    #       could probably avoid instantiating or constructing the
    #       response until we know we need it.
    resp_cc = self.parse_cache_control(headers)

    # determine freshness
    freshness_lifetime = 0

    # Check the max-age pragma in the cache control header
    max_age = resp_cc.get(&quot;max-age&quot;)
    if max_age is not None:
        freshness_lifetime = max_age
        logger.debug(&quot;Freshness lifetime from max-age: %i&quot;, freshness_lifetime)

    # If there isn&#39;t a max-age, check for an expires header
    elif &quot;expires&quot; in headers:
        expires = parsedate_tz(headers[&quot;expires&quot;])
        if expires is not None:
            expire_time = calendar.timegm(expires[:6]) - date
            freshness_lifetime = max(0, expire_time)
            logger.debug(&quot;Freshness lifetime from expires: %i&quot;, freshness_lifetime)

    # Determine if we are setting freshness limit in the
    # request. Note, this overrides what was in the response.
    max_age = cc.get(&quot;max-age&quot;)
    if max_age is not None:
        freshness_lifetime = max_age
        logger.debug(
            &quot;Freshness lifetime from request max-age: %i&quot;, freshness_lifetime
        )

    min_fresh = cc.get(&quot;min-fresh&quot;)
    if min_fresh is not None:
        # adjust our current age by our min fresh
        current_age += min_fresh
        logger.debug(&quot;Adjusted current age from min-fresh: %i&quot;, current_age)

    # Return entry if it is fresh enough
    if freshness_lifetime &gt; current_age:
        logger.debug(&#39;The response is &quot;fresh&quot;, returning cached response&#39;)
        logger.debug(&quot;%i &gt; %i&quot;, freshness_lifetime, current_age)
        return resp

    # we&#39;re not fresh. If we don&#39;t have an Etag, clear it out
    if &quot;etag&quot; not in headers:
        logger.debug(&#39;The cached response is &quot;stale&quot; with no etag, purging&#39;)
        self.cache.delete(cache_url)

    # return the original handler
    return False

def conditional_headers(self, request: PreparedRequest) -&gt; dict[str, str]:
    resp = self._load_from_cache(request)
    new_headers = {}

    if resp:
        headers: CaseInsensitiveDict[str] = CaseInsensitiveDict(resp.headers)

        if &quot;etag&quot; in headers:
            new_headers[&quot;If-None-Match&quot;] = headers[&quot;ETag&quot;]

        if &quot;last-modified&quot; in headers:
            new_headers[&quot;If-Modified-Since&quot;] = headers[&quot;Last-Modified&quot;]

    return new_headers

def _cache_set(
    self,
    cache_url: str,
    request: PreparedRequest,
    response: HTTPResponse,
    body: bytes | None = None,
    expires_time: int | None = None,
) -&gt; None:
    &quot;&quot;&quot;
    Store the data in the cache.
    &quot;&quot;&quot;
    if isinstance(self.cache, SeparateBodyBaseCache):
        # We pass in the body separately; just put a placeholder empty
        # string in the metadata.
        self.cache.set(
            cache_url,
            self.serializer.dumps(request, response, b&quot;&quot;),
            expires=expires_time,
        )
        # body is None can happen when, for example, we&#39;re only updating
        # headers, as is the case in update_cached_response().
        if body is not None:
            self.cache.set_body(cache_url, body)
    else:
        self.cache.set(
            cache_url,
            self.serializer.dumps(request, response, body),
            expires=expires_time,
        )

def cache_response(
    self,
    request: PreparedRequest,
    response_or_ref: HTTPResponse | weakref.ReferenceType[HTTPResponse],
    body: bytes | None = None,
    status_codes: Collection[int] | None = None,
) -&gt; None:
    &quot;&quot;&quot;
    Algorithm for caching requests.

    This assumes a requests Response object.
    &quot;&quot;&quot;
    if isinstance(response_or_ref, weakref.ReferenceType):
        response = response_or_ref()
        if response is None:
            # The weakref can be None only in case the user used streamed request
            # and did not consume or close it, and holds no reference to requests.Response.
            # In such case, we don&#39;t want to cache the response.
            return
    else:
        response = response_or_ref

    # From httplib2: Don&#39;t cache 206&#39;s since we aren&#39;t going to
    #                handle byte range requests
    cacheable_status_codes = status_codes or self.cacheable_status_codes
    if response.status not in cacheable_status_codes:
        logger.debug(
            &quot;Status code %s not in %s&quot;, response.status, cacheable_status_codes
        )
        return

    response_headers: CaseInsensitiveDict[str] = CaseInsensitiveDict(
        response.headers
    )

    if &quot;date&quot; in response_headers:
        time_tuple = parsedate_tz(response_headers[&quot;date&quot;])
        assert time_tuple is not None
        date = calendar.timegm(time_tuple[:6])
    else:
        date = 0

    # If we&#39;ve been given a body, our response has a Content-Length, that
    # Content-Length is valid then we can check to see if the body we&#39;ve
    # been given matches the expected size, and if it doesn&#39;t we&#39;ll just
    # skip trying to cache it.
    if (
        body is not None
        and &quot;content-length&quot; in response_headers
        and response_headers[&quot;content-length&quot;].isdigit()
        and int(response_headers[&quot;content-length&quot;]) != len(body)
    ):
        return

    cc_req = self.parse_cache_control(request.headers)
    cc = self.parse_cache_control(response_headers)

    assert request.url is not None
    cache_url = self.cache_url(request.url)
    logger.debug(&#39;Updating cache with response from &quot;%s&quot;&#39;, cache_url)

    # Delete it from the cache if we happen to have it stored there
    no_store = False
    if &quot;no-store&quot; in cc:
        no_store = True
        logger.debug(&#39;Response header has &quot;no-store&quot;&#39;)
    if &quot;no-store&quot; in cc_req:
        no_store = True
        logger.debug(&#39;Request header has &quot;no-store&quot;&#39;)
    if no_store and self.cache.get(cache_url):
        logger.debug(&#39;Purging existing cache entry to honor &quot;no-store&quot;&#39;)
        self.cache.delete(cache_url)
    if no_store:
        return

    # https://tools.ietf.org/html/rfc7234#section-4.1:
    # A Vary header field-value of &quot;*&quot; always fails to match.
    # Storing such a response leads to a deserialization warning
    # during cache lookup and is not allowed to ever be served,
    # so storing it can be avoided.
    if &quot;*&quot; in response_headers.get(&quot;vary&quot;, &quot;&quot;):
        logger.debug(&#39;Response header has &quot;Vary: *&quot;&#39;)
        return

    # If we&#39;ve been given an etag, then keep the response
    if self.cache_etags and &quot;etag&quot; in response_headers:
        expires_time = 0
        if response_headers.get(&quot;expires&quot;):
            expires = parsedate_tz(response_headers[&quot;expires&quot;])
            if expires is not None:
                expires_time = calendar.timegm(expires[:6]) - date

        expires_time = max(expires_time, 14 * 86400)

        logger.debug(f&quot;etag object cached for {expires_time} seconds&quot;)
        logger.debug(&quot;Caching due to etag&quot;)
        self._cache_set(cache_url, request, response, body, expires_time)

    # Add to the cache any permanent redirects. We do this before looking
    # that the Date headers.
    elif int(response.status) in PERMANENT_REDIRECT_STATUSES:
        logger.debug(&quot;Caching permanent redirect&quot;)
        self._cache_set(cache_url, request, response, b&quot;&quot;)

    # Add to the cache if the response headers demand it. If there
    # is no date header then we can&#39;t do anything about expiring
    # the cache.
    elif &quot;date&quot; in response_headers:
        time_tuple = parsedate_tz(response_headers[&quot;date&quot;])
        assert time_tuple is not None
        date = calendar.timegm(time_tuple[:6])
        # cache when there is a max-age &gt; 0
        max_age = cc.get(&quot;max-age&quot;)
        if max_age is not None and max_age &gt; 0:
            logger.debug(&quot;Caching b/c date exists and max-age &gt; 0&quot;)
            expires_time = max_age
            self._cache_set(
                cache_url,
                request,
                response,
                body,
                expires_time,
            )

        # If the request can expire, it means we should cache it
        # in the meantime.
        elif &quot;expires&quot; in response_headers:
            if response_headers[&quot;expires&quot;]:
                expires = parsedate_tz(response_headers[&quot;expires&quot;])
                if expires is not None:
                    expires_time = calendar.timegm(expires[:6]) - date
                else:
                    expires_time = None

                logger.debug(
                    &quot;Caching b/c of expires header. expires in {} seconds&quot;.format(
                        expires_time
                    )
                )
                self._cache_set(
                    cache_url,
                    request,
                    response,
                    body,
                    expires_time,
                )

def update_cached_response(
    self, request: PreparedRequest, response: HTTPResponse
) -&gt; HTTPResponse:
    &quot;&quot;&quot;On a 304 we will get a new set of headers that we want to
    update our cached value with, assuming we have one.

    This should only ever be called when we&#39;ve sent an ETag and
    gotten a 304 as the response.
    &quot;&quot;&quot;
    assert request.url is not None
    cache_url = self.cache_url(request.url)
    cached_response = self._load_from_cache(request)

    if not cached_response:
        # we didn&#39;t have a cached response
        return response

    # Lets update our headers with the headers from the new request:
    # http://tools.ietf.org/html/draft-ietf-httpbis-p4-conditional-26#section-4.1
    #
    # The server isn&#39;t supposed to send headers that would make
    # the cached body invalid. But... just in case, we&#39;ll be sure
    # to strip out ones we know that might be problmatic due to
    # typical assumptions.
    excluded_headers = [&quot;content-length&quot;]

    cached_response.headers.update(
        {
            k: v
            for k, v in response.headers.items()
            if k.lower() not in excluded_headers
        }
    )

    # we want a 200 b/c we have content via the cache
    cached_response.status = 200

    # update our cache
    self._cache_set(cache_url, request, cached_response)

    return cached_response
</pre></div>
</div>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Adam Worsnip.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>