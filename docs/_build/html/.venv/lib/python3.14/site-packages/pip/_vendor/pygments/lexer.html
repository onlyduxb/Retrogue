

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>—————————————————————————— &mdash; Retrogue 0.0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../_static/custom.css?v=a6a68382" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../_static/fonts.css?v=5583d106" />

  
      <script src="../../../../../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../../../../../_static/documentation_options.js?v=d45e8c67"></script>
      <script src="../../../../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../../../index.html" class="icon icon-home">
            Retrogue
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../modules.html">src</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../../index.html">Retrogue</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">——————————————————————————</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../../_sources/.venv/lib/python3.14/site-packages/pip/_vendor/pygments/lexer.py.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <p>“””
pygments.lexer
~~~~~~~~~~~~~~</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Base lexer classes.

:copyright: Copyright 2006-2025 by the Pygments team, see AUTHORS.
:license: BSD, see LICENSE for details.
</pre></div>
</div>
<p>“””</p>
<p>import re
import sys
import time</p>
<p>from pip._vendor.pygments.filter import apply_filters, Filter
from pip._vendor.pygments.filters import get_filter_by_name
from pip._vendor.pygments.token import Error, Text, Other, Whitespace, _TokenType
from pip._vendor.pygments.util import get_bool_opt, get_int_opt, get_list_opt, <br />
make_analysator, Future, guess_decode
from pip._vendor.pygments.regexopt import regex_opt</p>
<p><strong>all</strong> = [‘Lexer’, ‘RegexLexer’, ‘ExtendedRegexLexer’, ‘DelegatingLexer’,
‘LexerContext’, ‘include’, ‘inherit’, ‘bygroups’, ‘using’, ‘this’,
‘default’, ‘words’, ‘line_re’]</p>
<p>line_re = re.compile(‘.*?\n’)</p>
<p>_encoding_map = [(b’\xef\xbb\xbf’, ‘utf-8’),
(b’\xff\xfe\0\0’, ‘utf-32’),
(b’\0\0\xfe\xff’, ‘utf-32be’),
(b’\xff\xfe’, ‘utf-16’),
(b’\xfe\xff’, ‘utf-16be’)]</p>
<p>_default_analyse = staticmethod(lambda x: 0.0)</p>
<p>class LexerMeta(type):
“””
This metaclass automagically converts <code class="docutils literal notranslate"><span class="pre">analyse_text</span></code> methods into
static methods which always return float values.
“””</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>def __new__(mcs, name, bases, d):
    if &#39;analyse_text&#39; in d:
        d[&#39;analyse_text&#39;] = make_analysator(d[&#39;analyse_text&#39;])
    return type.__new__(mcs, name, bases, d)
</pre></div>
</div>
<p>class Lexer(metaclass=LexerMeta):
“””
Lexer for a specific language.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>See also :doc:`lexerdevelopment`, a high-level guide to writing
lexers.

Lexer classes have attributes used for choosing the most appropriate
lexer based on various criteria.

.. autoattribute:: name
   :no-value:
.. autoattribute:: aliases
   :no-value:
.. autoattribute:: filenames
   :no-value:
.. autoattribute:: alias_filenames
.. autoattribute:: mimetypes
   :no-value:
.. autoattribute:: priority

Lexers included in Pygments should have two additional attributes:

.. autoattribute:: url
   :no-value:
.. autoattribute:: version_added
   :no-value:

Lexers included in Pygments may have additional attributes:

.. autoattribute:: _example
   :no-value:

You can pass options to the constructor. The basic options recognized
by all lexers and processed by the base `Lexer` class are:

``stripnl``
    Strip leading and trailing newlines from the input (default: True).
``stripall``
    Strip all leading and trailing whitespace from the input
    (default: False).
``ensurenl``
    Make sure that the input ends with a newline (default: True).  This
    is required for some lexers that consume input linewise.

    .. versionadded:: 1.3

``tabsize``
    If given and greater than 0, expand tabs in the input (default: 0).
``encoding``
    If given, must be an encoding name. This encoding will be used to
    convert the input string to Unicode, if it is not already a Unicode
    string (default: ``&#39;guess&#39;``, which uses a simple UTF-8 / Locale /
    Latin1 detection.  Can also be ``&#39;chardet&#39;`` to use the chardet
    library, if it is installed.
``inencoding``
    Overrides the ``encoding`` if given.
&quot;&quot;&quot;

#: Full name of the lexer, in human-readable form
name = None

#: A list of short, unique identifiers that can be used to look
#: up the lexer from a list, e.g., using `get_lexer_by_name()`.
aliases = []

#: A list of `fnmatch` patterns that match filenames which contain
#: content for this lexer. The patterns in this list should be unique among
#: all lexers.
filenames = []

#: A list of `fnmatch` patterns that match filenames which may or may not
#: contain content for this lexer. This list is used by the
#: :func:`.guess_lexer_for_filename()` function, to determine which lexers
#: are then included in guessing the correct one. That means that
#: e.g. every lexer for HTML and a template language should include
#: ``\*.html`` in this list.
alias_filenames = []

#: A list of MIME types for content that can be lexed with this lexer.
mimetypes = []

#: Priority, should multiple lexers match and no content is provided
priority = 0

#: URL of the language specification/definition. Used in the Pygments
#: documentation. Set to an empty string to disable.
url = None

#: Version of Pygments in which the lexer was added.
version_added = None

#: Example file name. Relative to the ``tests/examplefiles`` directory.
#: This is used by the documentation generator to show an example.
_example = None

def __init__(self, **options):
    &quot;&quot;&quot;
    This constructor takes arbitrary options as keyword arguments.
    Every subclass must first process its own options and then call
    the `Lexer` constructor, since it processes the basic
    options like `stripnl`.

    An example looks like this:

    .. sourcecode:: python

       def __init__(self, **options):
           self.compress = options.get(&#39;compress&#39;, &#39;&#39;)
           Lexer.__init__(self, **options)

    As these options must all be specifiable as strings (due to the
    command line usage), there are various utility functions
    available to help with that, see `Utilities`_.
    &quot;&quot;&quot;
    self.options = options
    self.stripnl = get_bool_opt(options, &#39;stripnl&#39;, True)
    self.stripall = get_bool_opt(options, &#39;stripall&#39;, False)
    self.ensurenl = get_bool_opt(options, &#39;ensurenl&#39;, True)
    self.tabsize = get_int_opt(options, &#39;tabsize&#39;, 0)
    self.encoding = options.get(&#39;encoding&#39;, &#39;guess&#39;)
    self.encoding = options.get(&#39;inencoding&#39;) or self.encoding
    self.filters = []
    for filter_ in get_list_opt(options, &#39;filters&#39;, ()):
        self.add_filter(filter_)

def __repr__(self):
    if self.options:
        return f&#39;&lt;pygments.lexers.{self.__class__.__name__} with {self.options!r}&gt;&#39;
    else:
        return f&#39;&lt;pygments.lexers.{self.__class__.__name__}&gt;&#39;

def add_filter(self, filter_, **options):
    &quot;&quot;&quot;
    Add a new stream filter to this lexer.
    &quot;&quot;&quot;
    if not isinstance(filter_, Filter):
        filter_ = get_filter_by_name(filter_, **options)
    self.filters.append(filter_)

def analyse_text(text):
    &quot;&quot;&quot;
    A static method which is called for lexer guessing.

    It should analyse the text and return a float in the range
    from ``0.0`` to ``1.0``.  If it returns ``0.0``, the lexer
    will not be selected as the most probable one, if it returns
    ``1.0``, it will be selected immediately.  This is used by
    `guess_lexer`.

    The `LexerMeta` metaclass automatically wraps this function so
    that it works like a static method (no ``self`` or ``cls``
    parameter) and the return value is automatically converted to
    `float`. If the return value is an object that is boolean `False`
    it&#39;s the same as if the return values was ``0.0``.
    &quot;&quot;&quot;

def _preprocess_lexer_input(self, text):
    &quot;&quot;&quot;Apply preprocessing such as decoding the input, removing BOM and normalizing newlines.&quot;&quot;&quot;

    if not isinstance(text, str):
        if self.encoding == &#39;guess&#39;:
            text, _ = guess_decode(text)
        elif self.encoding == &#39;chardet&#39;:
            try:
                # pip vendoring note: this code is not reachable by pip,
                # removed import of chardet to make it clear.
                raise ImportError(&#39;chardet is not vendored by pip&#39;)
            except ImportError as e:
                raise ImportError(&#39;To enable chardet encoding guessing, &#39;
                                  &#39;please install the chardet library &#39;
                                  &#39;from http://chardet.feedparser.org/&#39;) from e
            # check for BOM first
            decoded = None
            for bom, encoding in _encoding_map:
                if text.startswith(bom):
                    decoded = text[len(bom):].decode(encoding, &#39;replace&#39;)
                    break
            # no BOM found, so use chardet
            if decoded is None:
                enc = chardet.detect(text[:1024])  # Guess using first 1KB
                decoded = text.decode(enc.get(&#39;encoding&#39;) or &#39;utf-8&#39;,
                                      &#39;replace&#39;)
            text = decoded
        else:
            text = text.decode(self.encoding)
            if text.startswith(&#39;\ufeff&#39;):
                text = text[len(&#39;\ufeff&#39;):]
    else:
        if text.startswith(&#39;\ufeff&#39;):
            text = text[len(&#39;\ufeff&#39;):]

    # text now *is* a unicode string
    text = text.replace(&#39;\r\n&#39;, &#39;\n&#39;)
    text = text.replace(&#39;\r&#39;, &#39;\n&#39;)
    if self.stripall:
        text = text.strip()
    elif self.stripnl:
        text = text.strip(&#39;\n&#39;)
    if self.tabsize &gt; 0:
        text = text.expandtabs(self.tabsize)
    if self.ensurenl and not text.endswith(&#39;\n&#39;):
        text += &#39;\n&#39;

    return text

def get_tokens(self, text, unfiltered=False):
    &quot;&quot;&quot;
    This method is the basic interface of a lexer. It is called by
    the `highlight()` function. It must process the text and return an
    iterable of ``(tokentype, value)`` pairs from `text`.

    Normally, you don&#39;t need to override this method. The default
    implementation processes the options recognized by all lexers
    (`stripnl`, `stripall` and so on), and then yields all tokens
    from `get_tokens_unprocessed()`, with the ``index`` dropped.

    If `unfiltered` is set to `True`, the filtering mechanism is
    bypassed even if filters are defined.
    &quot;&quot;&quot;
    text = self._preprocess_lexer_input(text)

    def streamer():
        for _, t, v in self.get_tokens_unprocessed(text):
            yield t, v
    stream = streamer()
    if not unfiltered:
        stream = apply_filters(stream, self.filters, self)
    return stream

def get_tokens_unprocessed(self, text):
    &quot;&quot;&quot;
    This method should process the text and return an iterable of
    ``(index, tokentype, value)`` tuples where ``index`` is the starting
    position of the token within the input text.

    It must be overridden by subclasses. It is recommended to
    implement it as a generator to maximize effectiveness.
    &quot;&quot;&quot;
    raise NotImplementedError
</pre></div>
</div>
<p>class DelegatingLexer(Lexer):
“””
This lexer takes two lexer as arguments. A root lexer and
a language lexer. First everything is scanned using the language
lexer, afterwards all <code class="docutils literal notranslate"><span class="pre">Other</span></code> tokens are lexed using the root
lexer.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>The lexers from the ``template`` lexer package use this base lexer.
&quot;&quot;&quot;

def __init__(self, _root_lexer, _language_lexer, _needle=Other, **options):
    self.root_lexer = _root_lexer(**options)
    self.language_lexer = _language_lexer(**options)
    self.needle = _needle
    Lexer.__init__(self, **options)

def get_tokens_unprocessed(self, text):
    buffered = &#39;&#39;
    insertions = []
    lng_buffer = []
    for i, t, v in self.language_lexer.get_tokens_unprocessed(text):
        if t is self.needle:
            if lng_buffer:
                insertions.append((len(buffered), lng_buffer))
                lng_buffer = []
            buffered += v
        else:
            lng_buffer.append((i, t, v))
    if lng_buffer:
        insertions.append((len(buffered), lng_buffer))
    return do_insertions(insertions,
                         self.root_lexer.get_tokens_unprocessed(buffered))
</pre></div>
</div>
<section id="id1">
<h1>——————————————————————————<a class="headerlink" href="#id1" title="Link to this heading"></a></h1>
</section>
<section id="regexlexer-and-extendedregexlexer">
<h1>RegexLexer and ExtendedRegexLexer<a class="headerlink" href="#regexlexer-and-extendedregexlexer" title="Link to this heading"></a></h1>
</section>
<section id="id2">
<h1><a class="headerlink" href="#id2" title="Link to this heading"></a></h1>
<p>class include(str):  # pylint: disable=invalid-name
“””
Indicates that a state should include rules from another state.
“””
pass</p>
<p>class _inherit:
“””
Indicates the a state should inherit from its superclass.
“””
def <strong>repr</strong>(self):
return ‘inherit’</p>
<p>inherit = _inherit()  # pylint: disable=invalid-name</p>
<p>class combined(tuple):  # pylint: disable=invalid-name
“””
Indicates a state combined from multiple states.
“””</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>def __new__(cls, *args):
    return tuple.__new__(cls, args)

def __init__(self, *args):
    # tuple.__init__ doesn&#39;t do anything
    pass
</pre></div>
</div>
<p>class _PseudoMatch:
“””
A pseudo match object constructed from a string.
“””</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>def __init__(self, start, text):
    self._text = text
    self._start = start

def start(self, arg=None):
    return self._start

def end(self, arg=None):
    return self._start + len(self._text)

def group(self, arg=None):
    if arg:
        raise IndexError(&#39;No such group&#39;)
    return self._text

def groups(self):
    return (self._text,)

def groupdict(self):
    return {}
</pre></div>
</div>
<p>def bygroups(*args):
“””
Callback that yields multiple actions for each group in the match.
“””
def callback(lexer, match, ctx=None):
for i, action in enumerate(args):
if action is None:
continue
elif type(action) is _TokenType:
data = match.group(i + 1)
if data:
yield match.start(i + 1), action, data
else:
data = match.group(i + 1)
if data is not None:
if ctx:
ctx.pos = match.start(i + 1)
for item in action(lexer,
_PseudoMatch(match.start(i + 1), data), ctx):
if item:
yield item
if ctx:
ctx.pos = match.end()
return callback</p>
<p>class _This:
“””
Special singleton used for indicating the caller class.
Used by <code class="docutils literal notranslate"><span class="pre">using</span></code>.
“””</p>
<p>this = _This()</p>
<p>def using(_other, **kwargs):
“””
Callback that processes the match with a different lexer.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>The keyword arguments are forwarded to the lexer, except `state` which
is handled separately.

`state` specifies the state that the new lexer will start in, and can
be an enumerable such as (&#39;root&#39;, &#39;inline&#39;, &#39;string&#39;) or a simple
string which is assumed to be on top of the root state.

Note: For that to work, `_other` must not be an `ExtendedRegexLexer`.
&quot;&quot;&quot;
gt_kwargs = {}
if &#39;state&#39; in kwargs:
    s = kwargs.pop(&#39;state&#39;)
    if isinstance(s, (list, tuple)):
        gt_kwargs[&#39;stack&#39;] = s
    else:
        gt_kwargs[&#39;stack&#39;] = (&#39;root&#39;, s)

if _other is this:
    def callback(lexer, match, ctx=None):
        # if keyword arguments are given the callback
        # function has to create a new lexer instance
        if kwargs:
            # XXX: cache that somehow
            kwargs.update(lexer.options)
            lx = lexer.__class__(**kwargs)
        else:
            lx = lexer
        s = match.start()
        for i, t, v in lx.get_tokens_unprocessed(match.group(), **gt_kwargs):
            yield i + s, t, v
        if ctx:
            ctx.pos = match.end()
else:
    def callback(lexer, match, ctx=None):
        # XXX: cache that somehow
        kwargs.update(lexer.options)
        lx = _other(**kwargs)

        s = match.start()
        for i, t, v in lx.get_tokens_unprocessed(match.group(), **gt_kwargs):
            yield i + s, t, v
        if ctx:
            ctx.pos = match.end()
return callback
</pre></div>
</div>
<p>class default:
“””
Indicates a state or state action (e.g. #pop) to apply.
For example default(‘#pop’) is equivalent to (‘’, Token, ‘#pop’)
Note that state tuples may be used as well.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>.. versionadded:: 2.0
&quot;&quot;&quot;
def __init__(self, state):
    self.state = state
</pre></div>
</div>
<p>class words(Future):
“””
Indicates a list of literal words that is transformed into an optimized
regex that matches any of the words.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>.. versionadded:: 2.0
&quot;&quot;&quot;
def __init__(self, words, prefix=&#39;&#39;, suffix=&#39;&#39;):
    self.words = words
    self.prefix = prefix
    self.suffix = suffix

def get(self):
    return regex_opt(self.words, prefix=self.prefix, suffix=self.suffix)
</pre></div>
</div>
<p>class RegexLexerMeta(LexerMeta):
“””
Metaclass for RegexLexer, creates the self._tokens attribute from
self.tokens on the first instantiation.
“””</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>def _process_regex(cls, regex, rflags, state):
    &quot;&quot;&quot;Preprocess the regular expression component of a token definition.&quot;&quot;&quot;
    if isinstance(regex, Future):
        regex = regex.get()
    return re.compile(regex, rflags).match

def _process_token(cls, token):
    &quot;&quot;&quot;Preprocess the token component of a token definition.&quot;&quot;&quot;
    assert type(token) is _TokenType or callable(token), \
        f&#39;token type must be simple type or callable, not {token!r}&#39;
    return token

def _process_new_state(cls, new_state, unprocessed, processed):
    &quot;&quot;&quot;Preprocess the state transition action of a token definition.&quot;&quot;&quot;
    if isinstance(new_state, str):
        # an existing state
        if new_state == &#39;#pop&#39;:
            return -1
        elif new_state in unprocessed:
            return (new_state,)
        elif new_state == &#39;#push&#39;:
            return new_state
        elif new_state[:5] == &#39;#pop:&#39;:
            return -int(new_state[5:])
        else:
            assert False, f&#39;unknown new state {new_state!r}&#39;
    elif isinstance(new_state, combined):
        # combine a new state from existing ones
        tmp_state = &#39;_tmp_%d&#39; % cls._tmpname
        cls._tmpname += 1
        itokens = []
        for istate in new_state:
            assert istate != new_state, f&#39;circular state ref {istate!r}&#39;
            itokens.extend(cls._process_state(unprocessed,
                                              processed, istate))
        processed[tmp_state] = itokens
        return (tmp_state,)
    elif isinstance(new_state, tuple):
        # push more than one state
        for istate in new_state:
            assert (istate in unprocessed or
                    istate in (&#39;#pop&#39;, &#39;#push&#39;)), \
                &#39;unknown new state &#39; + istate
        return new_state
    else:
        assert False, f&#39;unknown new state def {new_state!r}&#39;

def _process_state(cls, unprocessed, processed, state):
    &quot;&quot;&quot;Preprocess a single state definition.&quot;&quot;&quot;
    assert isinstance(state, str), f&quot;wrong state name {state!r}&quot;
    assert state[0] != &#39;#&#39;, f&quot;invalid state name {state!r}&quot;
    if state in processed:
        return processed[state]
    tokens = processed[state] = []
    rflags = cls.flags
    for tdef in unprocessed[state]:
        if isinstance(tdef, include):
            # it&#39;s a state reference
            assert tdef != state, f&quot;circular state reference {state!r}&quot;
            tokens.extend(cls._process_state(unprocessed, processed,
                                             str(tdef)))
            continue
        if isinstance(tdef, _inherit):
            # should be processed already, but may not in the case of:
            # 1. the state has no counterpart in any parent
            # 2. the state includes more than one &#39;inherit&#39;
            continue
        if isinstance(tdef, default):
            new_state = cls._process_new_state(tdef.state, unprocessed, processed)
            tokens.append((re.compile(&#39;&#39;).match, None, new_state))
            continue

        assert type(tdef) is tuple, f&quot;wrong rule def {tdef!r}&quot;

        try:
            rex = cls._process_regex(tdef[0], rflags, state)
        except Exception as err:
            raise ValueError(f&quot;uncompilable regex {tdef[0]!r} in state {state!r} of {cls!r}: {err}&quot;) from err

        token = cls._process_token(tdef[1])

        if len(tdef) == 2:
            new_state = None
        else:
            new_state = cls._process_new_state(tdef[2],
                                               unprocessed, processed)

        tokens.append((rex, token, new_state))
    return tokens

def process_tokendef(cls, name, tokendefs=None):
    &quot;&quot;&quot;Preprocess a dictionary of token definitions.&quot;&quot;&quot;
    processed = cls._all_tokens[name] = {}
    tokendefs = tokendefs or cls.tokens[name]
    for state in list(tokendefs):
        cls._process_state(tokendefs, processed, state)
    return processed

def get_tokendefs(cls):
    &quot;&quot;&quot;
    Merge tokens from superclasses in MRO order, returning a single tokendef
    dictionary.

    Any state that is not defined by a subclass will be inherited
    automatically.  States that *are* defined by subclasses will, by
    default, override that state in the superclass.  If a subclass wishes to
    inherit definitions from a superclass, it can use the special value
    &quot;inherit&quot;, which will cause the superclass&#39; state definition to be
    included at that point in the state.
    &quot;&quot;&quot;
    tokens = {}
    inheritable = {}
    for c in cls.__mro__:
        toks = c.__dict__.get(&#39;tokens&#39;, {})

        for state, items in toks.items():
            curitems = tokens.get(state)
            if curitems is None:
                # N.b. because this is assigned by reference, sufficiently
                # deep hierarchies are processed incrementally (e.g. for
                # A(B), B(C), C(RegexLexer), B will be premodified so X(B)
                # will not see any inherits in B).
                tokens[state] = items
                try:
                    inherit_ndx = items.index(inherit)
                except ValueError:
                    continue
                inheritable[state] = inherit_ndx
                continue

            inherit_ndx = inheritable.pop(state, None)
            if inherit_ndx is None:
                continue

            # Replace the &quot;inherit&quot; value with the items
            curitems[inherit_ndx:inherit_ndx+1] = items
            try:
                # N.b. this is the index in items (that is, the superclass
                # copy), so offset required when storing below.
                new_inh_ndx = items.index(inherit)
            except ValueError:
                pass
            else:
                inheritable[state] = inherit_ndx + new_inh_ndx

    return tokens

def __call__(cls, *args, **kwds):
    &quot;&quot;&quot;Instantiate cls after preprocessing its token definitions.&quot;&quot;&quot;
    if &#39;_tokens&#39; not in cls.__dict__:
        cls._all_tokens = {}
        cls._tmpname = 0
        if hasattr(cls, &#39;token_variants&#39;) and cls.token_variants:
            # don&#39;t process yet
            pass
        else:
            cls._tokens = cls.process_tokendef(&#39;&#39;, cls.get_tokendefs())

    return type.__call__(cls, *args, **kwds)
</pre></div>
</div>
<p>class RegexLexer(Lexer, metaclass=RegexLexerMeta):
“””
Base for simple stateful regular expression-based lexers.
Simplifies the lexing process so that you need only
provide a list of states and regular expressions.
“””</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>#: Flags for compiling the regular expressions.
#: Defaults to MULTILINE.
flags = re.MULTILINE

#: At all time there is a stack of states. Initially, the stack contains
#: a single state &#39;root&#39;. The top of the stack is called &quot;the current state&quot;.
#:
#: Dict of ``{&#39;state&#39;: [(regex, tokentype, new_state), ...], ...}``
#:
#: ``new_state`` can be omitted to signify no state transition.
#: If ``new_state`` is a string, it is pushed on the stack. This ensure
#: the new current state is ``new_state``.
#: If ``new_state`` is a tuple of strings, all of those strings are pushed
#: on the stack and the current state will be the last element of the list.
#: ``new_state`` can also be ``combined(&#39;state1&#39;, &#39;state2&#39;, ...)``
#: to signify a new, anonymous state combined from the rules of two
#: or more existing ones.
#: Furthermore, it can be &#39;#pop&#39; to signify going back one step in
#: the state stack, or &#39;#push&#39; to push the current state on the stack
#: again. Note that if you push while in a combined state, the combined
#: state itself is pushed, and not only the state in which the rule is
#: defined.
#:
#: The tuple can also be replaced with ``include(&#39;state&#39;)``, in which
#: case the rules from the state named by the string are included in the
#: current one.
tokens = {}

def get_tokens_unprocessed(self, text, stack=(&#39;root&#39;,)):
    &quot;&quot;&quot;
    Split ``text`` into (tokentype, text) pairs.

    ``stack`` is the initial stack (default: ``[&#39;root&#39;]``)
    &quot;&quot;&quot;
    pos = 0
    tokendefs = self._tokens
    statestack = list(stack)
    statetokens = tokendefs[statestack[-1]]
    while 1:
        for rexmatch, action, new_state in statetokens:
            m = rexmatch(text, pos)
            if m:
                if action is not None:
                    if type(action) is _TokenType:
                        yield pos, action, m.group()
                    else:
                        yield from action(self, m)
                pos = m.end()
                if new_state is not None:
                    # state transition
                    if isinstance(new_state, tuple):
                        for state in new_state:
                            if state == &#39;#pop&#39;:
                                if len(statestack) &gt; 1:
                                    statestack.pop()
                            elif state == &#39;#push&#39;:
                                statestack.append(statestack[-1])
                            else:
                                statestack.append(state)
                    elif isinstance(new_state, int):
                        # pop, but keep at least one state on the stack
                        # (random code leading to unexpected pops should
                        # not allow exceptions)
                        if abs(new_state) &gt;= len(statestack):
                            del statestack[1:]
                        else:
                            del statestack[new_state:]
                    elif new_state == &#39;#push&#39;:
                        statestack.append(statestack[-1])
                    else:
                        assert False, f&quot;wrong state def: {new_state!r}&quot;
                    statetokens = tokendefs[statestack[-1]]
                break
        else:
            # We are here only if all state tokens have been considered
            # and there was not a match on any of them.
            try:
                if text[pos] == &#39;\n&#39;:
                    # at EOL, reset state to &quot;root&quot;
                    statestack = [&#39;root&#39;]
                    statetokens = tokendefs[&#39;root&#39;]
                    yield pos, Whitespace, &#39;\n&#39;
                    pos += 1
                    continue
                yield pos, Error, text[pos]
                pos += 1
            except IndexError:
                break
</pre></div>
</div>
<p>class LexerContext:
“””
A helper object that holds lexer position data.
“””</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>def __init__(self, text, pos, stack=None, end=None):
    self.text = text
    self.pos = pos
    self.end = end or len(text)  # end=0 not supported ;-)
    self.stack = stack or [&#39;root&#39;]

def __repr__(self):
    return f&#39;LexerContext({self.text!r}, {self.pos!r}, {self.stack!r})&#39;
</pre></div>
</div>
<p>class ExtendedRegexLexer(RegexLexer):
“””
A RegexLexer that uses a context object to store its state.
“””</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>def get_tokens_unprocessed(self, text=None, context=None):
    &quot;&quot;&quot;
    Split ``text`` into (tokentype, text) pairs.
    If ``context`` is given, use this lexer context instead.
    &quot;&quot;&quot;
    tokendefs = self._tokens
    if not context:
        ctx = LexerContext(text, 0)
        statetokens = tokendefs[&#39;root&#39;]
    else:
        ctx = context
        statetokens = tokendefs[ctx.stack[-1]]
        text = ctx.text
    while 1:
        for rexmatch, action, new_state in statetokens:
            m = rexmatch(text, ctx.pos, ctx.end)
            if m:
                if action is not None:
                    if type(action) is _TokenType:
                        yield ctx.pos, action, m.group()
                        ctx.pos = m.end()
                    else:
                        yield from action(self, m, ctx)
                        if not new_state:
                            # altered the state stack?
                            statetokens = tokendefs[ctx.stack[-1]]
                # CAUTION: callback must set ctx.pos!
                if new_state is not None:
                    # state transition
                    if isinstance(new_state, tuple):
                        for state in new_state:
                            if state == &#39;#pop&#39;:
                                if len(ctx.stack) &gt; 1:
                                    ctx.stack.pop()
                            elif state == &#39;#push&#39;:
                                ctx.stack.append(ctx.stack[-1])
                            else:
                                ctx.stack.append(state)
                    elif isinstance(new_state, int):
                        # see RegexLexer for why this check is made
                        if abs(new_state) &gt;= len(ctx.stack):
                            del ctx.stack[1:]
                        else:
                            del ctx.stack[new_state:]
                    elif new_state == &#39;#push&#39;:
                        ctx.stack.append(ctx.stack[-1])
                    else:
                        assert False, f&quot;wrong state def: {new_state!r}&quot;
                    statetokens = tokendefs[ctx.stack[-1]]
                break
        else:
            try:
                if ctx.pos &gt;= ctx.end:
                    break
                if text[ctx.pos] == &#39;\n&#39;:
                    # at EOL, reset state to &quot;root&quot;
                    ctx.stack = [&#39;root&#39;]
                    statetokens = tokendefs[&#39;root&#39;]
                    yield ctx.pos, Text, &#39;\n&#39;
                    ctx.pos += 1
                    continue
                yield ctx.pos, Error, text[ctx.pos]
                ctx.pos += 1
            except IndexError:
                break
</pre></div>
</div>
<p>def do_insertions(insertions, tokens):
“””
Helper for lexers which must combine the results of several
sublexers.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>``insertions`` is a list of ``(index, itokens)`` pairs.
Each ``itokens`` iterable should be inserted at position
``index`` into the token stream given by the ``tokens``
argument.

The result is a combined token stream.

TODO: clean up the code here.
&quot;&quot;&quot;
insertions = iter(insertions)
try:
    index, itokens = next(insertions)
except StopIteration:
    # no insertions
    yield from tokens
    return

realpos = None
insleft = True

# iterate over the token stream where we want to insert
# the tokens from the insertion list.
for i, t, v in tokens:
    # first iteration. store the position of first item
    if realpos is None:
        realpos = i
    oldi = 0
    while insleft and i + len(v) &gt;= index:
        tmpval = v[oldi:index - i]
        if tmpval:
            yield realpos, t, tmpval
            realpos += len(tmpval)
        for it_index, it_token, it_value in itokens:
            yield realpos, it_token, it_value
            realpos += len(it_value)
        oldi = index - i
        try:
            index, itokens = next(insertions)
        except StopIteration:
            insleft = False
            break  # not strictly necessary
    if oldi &lt; len(v):
        yield realpos, t, v[oldi:]
        realpos += len(v) - oldi

# leftover tokens
while insleft:
    # no normal tokens, set realpos to zero
    realpos = realpos or 0
    for p, t, v in itokens:
        yield realpos, t, v
        realpos += len(v)
    try:
        index, itokens = next(insertions)
    except StopIteration:
        insleft = False
        break  # not strictly necessary
</pre></div>
</div>
<p>class ProfilingRegexLexerMeta(RegexLexerMeta):
“””Metaclass for ProfilingRegexLexer, collects regex timing info.”””</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>def _process_regex(cls, regex, rflags, state):
    if isinstance(regex, words):
        rex = regex_opt(regex.words, prefix=regex.prefix,
                        suffix=regex.suffix)
    else:
        rex = regex
    compiled = re.compile(rex, rflags)

    def match_func(text, pos, endpos=sys.maxsize):
        info = cls._prof_data[-1].setdefault((state, rex), [0, 0.0])
        t0 = time.time()
        res = compiled.match(text, pos, endpos)
        t1 = time.time()
        info[0] += 1
        info[1] += t1 - t0
        return res
    return match_func
</pre></div>
</div>
<p>class ProfilingRegexLexer(RegexLexer, metaclass=ProfilingRegexLexerMeta):
“””Drop-in replacement for RegexLexer that does profiling of its regexes.”””</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>_prof_data = []
_prof_sort_index = 4  # defaults to time per call

def get_tokens_unprocessed(self, text, stack=(&#39;root&#39;,)):
    # this needs to be a stack, since using(this) will produce nested calls
    self.__class__._prof_data.append({})
    yield from RegexLexer.get_tokens_unprocessed(self, text, stack)
    rawdata = self.__class__._prof_data.pop()
    data = sorted(((s, repr(r).strip(&#39;u\&#39;&#39;).replace(&#39;\\\\&#39;, &#39;\\&#39;)[:65],
                    n, 1000 * t, 1000 * t / n)
                   for ((s, r), (n, t)) in rawdata.items()),
                  key=lambda x: x[self._prof_sort_index],
                  reverse=True)
    sum_total = sum(x[3] for x in data)

    print()
    print(&#39;Profiling result for %s lexing %d chars in %.3f ms&#39; %
          (self.__class__.__name__, len(text), sum_total))
    print(&#39;=&#39; * 110)
    print(&#39;%-20s %-64s ncalls  tottime  percall&#39; % (&#39;state&#39;, &#39;regex&#39;))
    print(&#39;-&#39; * 110)
    for d in data:
        print(&#39;%-20s %-65s %5d %8.4f %8.4f&#39; % d)
    print(&#39;=&#39; * 110)
</pre></div>
</div>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Adam Worsnip.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>