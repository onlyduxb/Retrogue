

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>&lt;no title&gt; &mdash; Retrogue 0.0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../_static/custom.css?v=a6a68382" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../_static/fonts.css?v=5583d106" />

  
      <script src="../../../../../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../../../../../_static/documentation_options.js?v=d45e8c67"></script>
      <script src="../../../../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../../../index.html" class="icon icon-home">
            Retrogue
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../modules.html">src</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../../index.html">Retrogue</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">&lt;no title&gt;</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../../_sources/.venv/lib/python3.14/site-packages/pip/_vendor/requests/utils.py.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <p>“””
requests.utils</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>
This module provides utility functions that are used within Requests
that are also useful for external consumption.
&quot;&quot;&quot;

import codecs
import contextlib
import io
import os
import re
import socket
import struct
import sys
import tempfile
import warnings
import zipfile
from collections import OrderedDict

from pip._vendor.urllib3.util import make_headers, parse_url

from . import certs
from .__version__ import __version__

# to_native_string is unused here, but imported here for backwards compatibility
from ._internal_utils import (  # noqa: F401
    _HEADER_VALIDATORS_BYTE,
    _HEADER_VALIDATORS_STR,
    HEADER_VALIDATORS,
    to_native_string,
)
from .compat import (
    Mapping,
    basestring,
    bytes,
    getproxies,
    getproxies_environment,
    integer_types,
    is_urllib3_1,
)
from .compat import parse_http_list as _parse_list_header
from .compat import (
    proxy_bypass,
    proxy_bypass_environment,
    quote,
    str,
    unquote,
    urlparse,
    urlunparse,
)
from .cookies import cookiejar_from_dict
from .exceptions import (
    FileModeWarning,
    InvalidHeader,
    InvalidURL,
    UnrewindableBodyError,
)
from .structures import CaseInsensitiveDict

NETRC_FILES = (&quot;.netrc&quot;, &quot;_netrc&quot;)

DEFAULT_CA_BUNDLE_PATH = certs.where()

DEFAULT_PORTS = {&quot;http&quot;: 80, &quot;https&quot;: 443}

# Ensure that &#39;, &#39; is used to preserve previous delimiter behavior.
DEFAULT_ACCEPT_ENCODING = &quot;, &quot;.join(
    re.split(r&quot;,\s*&quot;, make_headers(accept_encoding=True)[&quot;accept-encoding&quot;])
)


if sys.platform == &quot;win32&quot;:
    # provide a proxy_bypass version on Windows without DNS lookups

    def proxy_bypass_registry(host):
        try:
            import winreg
        except ImportError:
            return False

        try:
            internetSettings = winreg.OpenKey(
                winreg.HKEY_CURRENT_USER,
                r&quot;Software\Microsoft\Windows\CurrentVersion\Internet Settings&quot;,
            )
            # ProxyEnable could be REG_SZ or REG_DWORD, normalizing it
            proxyEnable = int(winreg.QueryValueEx(internetSettings, &quot;ProxyEnable&quot;)[0])
            # ProxyOverride is almost always a string
            proxyOverride = winreg.QueryValueEx(internetSettings, &quot;ProxyOverride&quot;)[0]
        except (OSError, ValueError):
            return False
        if not proxyEnable or not proxyOverride:
            return False

        # make a check value list from the registry entry: replace the
        # &#39;&lt;local&gt;&#39; string by the localhost entry and the corresponding
        # canonical entry.
        proxyOverride = proxyOverride.split(&quot;;&quot;)
        # filter out empty strings to avoid re.match return true in the following code.
        proxyOverride = filter(None, proxyOverride)
        # now check if we match one of the registry values.
        for test in proxyOverride:
            if test == &quot;&lt;local&gt;&quot;:
                if &quot;.&quot; not in host:
                    return True
            test = test.replace(&quot;.&quot;, r&quot;\.&quot;)  # mask dots
            test = test.replace(&quot;*&quot;, r&quot;.*&quot;)  # change glob sequence
            test = test.replace(&quot;?&quot;, r&quot;.&quot;)  # change glob char
            if re.match(test, host, re.I):
                return True
        return False

    def proxy_bypass(host):  # noqa
        &quot;&quot;&quot;Return True, if the host should be bypassed.

        Checks proxy settings gathered from the environment, if specified,
        or the registry.
        &quot;&quot;&quot;
        if getproxies_environment():
            return proxy_bypass_environment(host)
        else:
            return proxy_bypass_registry(host)


def dict_to_sequence(d):
    &quot;&quot;&quot;Returns an internal sequence dictionary update.&quot;&quot;&quot;

    if hasattr(d, &quot;items&quot;):
        d = d.items()

    return d


def super_len(o):
    total_length = None
    current_position = 0

    if not is_urllib3_1 and isinstance(o, str):
        # urllib3 2.x+ treats all strings as utf-8 instead
        # of latin-1 (iso-8859-1) like http.client.
        o = o.encode(&quot;utf-8&quot;)

    if hasattr(o, &quot;__len__&quot;):
        total_length = len(o)

    elif hasattr(o, &quot;len&quot;):
        total_length = o.len

    elif hasattr(o, &quot;fileno&quot;):
        try:
            fileno = o.fileno()
        except (io.UnsupportedOperation, AttributeError):
            # AttributeError is a surprising exception, seeing as how we&#39;ve just checked
            # that `hasattr(o, &#39;fileno&#39;)`.  It happens for objects obtained via
            # `Tarfile.extractfile()`, per issue 5229.
            pass
        else:
            total_length = os.fstat(fileno).st_size

            # Having used fstat to determine the file length, we need to
            # confirm that this file was opened up in binary mode.
            if &quot;b&quot; not in o.mode:
                warnings.warn(
                    (
                        &quot;Requests has determined the content-length for this &quot;
                        &quot;request using the binary size of the file: however, the &quot;
                        &quot;file has been opened in text mode (i.e. without the &#39;b&#39; &quot;
                        &quot;flag in the mode). This may lead to an incorrect &quot;
                        &quot;content-length. In Requests 3.0, support will be removed &quot;
                        &quot;for files in text mode.&quot;
                    ),
                    FileModeWarning,
                )

    if hasattr(o, &quot;tell&quot;):
        try:
            current_position = o.tell()
        except OSError:
            # This can happen in some weird situations, such as when the file
            # is actually a special file descriptor like stdin. In this
            # instance, we don&#39;t know what the length is, so set it to zero and
            # let requests chunk it instead.
            if total_length is not None:
                current_position = total_length
        else:
            if hasattr(o, &quot;seek&quot;) and total_length is None:
                # StringIO and BytesIO have seek but no usable fileno
                try:
                    # seek to end of file
                    o.seek(0, 2)
                    total_length = o.tell()

                    # seek back to current position to support
                    # partially read file-like objects
                    o.seek(current_position or 0)
                except OSError:
                    total_length = 0

    if total_length is None:
        total_length = 0

    return max(0, total_length - current_position)


def get_netrc_auth(url, raise_errors=False):
    &quot;&quot;&quot;Returns the Requests tuple auth for a given url from netrc.&quot;&quot;&quot;

    netrc_file = os.environ.get(&quot;NETRC&quot;)
    if netrc_file is not None:
        netrc_locations = (netrc_file,)
    else:
        netrc_locations = (f&quot;~/{f}&quot; for f in NETRC_FILES)

    try:
        from netrc import NetrcParseError, netrc

        netrc_path = None

        for f in netrc_locations:
            loc = os.path.expanduser(f)
            if os.path.exists(loc):
                netrc_path = loc
                break

        # Abort early if there isn&#39;t one.
        if netrc_path is None:
            return

        ri = urlparse(url)
        host = ri.hostname

        try:
            _netrc = netrc(netrc_path).authenticators(host)
            if _netrc:
                # Return with login / password
                login_i = 0 if _netrc[0] else 1
                return (_netrc[login_i], _netrc[2])
        except (NetrcParseError, OSError):
            # If there was a parsing error or a permissions issue reading the file,
            # we&#39;ll just skip netrc auth unless explicitly asked to raise errors.
            if raise_errors:
                raise

    # App Engine hackiness.
    except (ImportError, AttributeError):
        pass


def guess_filename(obj):
    &quot;&quot;&quot;Tries to guess the filename of the given object.&quot;&quot;&quot;
    name = getattr(obj, &quot;name&quot;, None)
    if name and isinstance(name, basestring) and name[0] != &quot;&lt;&quot; and name[-1] != &quot;&gt;&quot;:
        return os.path.basename(name)


def extract_zipped_paths(path):
    &quot;&quot;&quot;Replace nonexistent paths that look like they refer to a member of a zip
    archive with the location of an extracted copy of the target, or else
    just return the provided path unchanged.
    &quot;&quot;&quot;
    if os.path.exists(path):
        # this is already a valid path, no need to do anything further
        return path

    # find the first valid part of the provided path and treat that as a zip archive
    # assume the rest of the path is the name of a member in the archive
    archive, member = os.path.split(path)
    while archive and not os.path.exists(archive):
        archive, prefix = os.path.split(archive)
        if not prefix:
            # If we don&#39;t check for an empty prefix after the split (in other words, archive remains unchanged after the split),
            # we _can_ end up in an infinite loop on a rare corner case affecting a small number of users
            break
        member = &quot;/&quot;.join([prefix, member])

    if not zipfile.is_zipfile(archive):
        return path

    zip_file = zipfile.ZipFile(archive)
    if member not in zip_file.namelist():
        return path

    # we have a valid zip archive and a valid member of that archive
    tmp = tempfile.gettempdir()
    extracted_path = os.path.join(tmp, member.split(&quot;/&quot;)[-1])
    if not os.path.exists(extracted_path):
        # use read + write to avoid the creating nested folders, we only want the file, avoids mkdir racing condition
        with atomic_open(extracted_path) as file_handler:
            file_handler.write(zip_file.read(member))
    return extracted_path


@contextlib.contextmanager
def atomic_open(filename):
    &quot;&quot;&quot;Write a file to the disk in an atomic fashion&quot;&quot;&quot;
    tmp_descriptor, tmp_name = tempfile.mkstemp(dir=os.path.dirname(filename))
    try:
        with os.fdopen(tmp_descriptor, &quot;wb&quot;) as tmp_handler:
            yield tmp_handler
        os.replace(tmp_name, filename)
    except BaseException:
        os.remove(tmp_name)
        raise


def from_key_val_list(value):
    &quot;&quot;&quot;Take an object and test to see if it can be represented as a
    dictionary. Unless it can not be represented as such, return an
    OrderedDict, e.g.,

    ::

        &gt;&gt;&gt; from_key_val_list([(&#39;key&#39;, &#39;val&#39;)])
        OrderedDict([(&#39;key&#39;, &#39;val&#39;)])
        &gt;&gt;&gt; from_key_val_list(&#39;string&#39;)
        Traceback (most recent call last):
        ...
        ValueError: cannot encode objects that are not 2-tuples
        &gt;&gt;&gt; from_key_val_list({&#39;key&#39;: &#39;val&#39;})
        OrderedDict([(&#39;key&#39;, &#39;val&#39;)])

    :rtype: OrderedDict
    &quot;&quot;&quot;
    if value is None:
        return None

    if isinstance(value, (str, bytes, bool, int)):
        raise ValueError(&quot;cannot encode objects that are not 2-tuples&quot;)

    return OrderedDict(value)


def to_key_val_list(value):
    &quot;&quot;&quot;Take an object and test to see if it can be represented as a
    dictionary. If it can be, return a list of tuples, e.g.,

    ::

        &gt;&gt;&gt; to_key_val_list([(&#39;key&#39;, &#39;val&#39;)])
        [(&#39;key&#39;, &#39;val&#39;)]
        &gt;&gt;&gt; to_key_val_list({&#39;key&#39;: &#39;val&#39;})
        [(&#39;key&#39;, &#39;val&#39;)]
        &gt;&gt;&gt; to_key_val_list(&#39;string&#39;)
        Traceback (most recent call last):
        ...
        ValueError: cannot encode objects that are not 2-tuples

    :rtype: list
    &quot;&quot;&quot;
    if value is None:
        return None

    if isinstance(value, (str, bytes, bool, int)):
        raise ValueError(&quot;cannot encode objects that are not 2-tuples&quot;)

    if isinstance(value, Mapping):
        value = value.items()

    return list(value)


# From mitsuhiko/werkzeug (used with permission).
def parse_list_header(value):
    &quot;&quot;&quot;Parse lists as described by RFC 2068 Section 2.

    In particular, parse comma-separated lists where the elements of
    the list may include quoted-strings.  A quoted-string could
    contain a comma.  A non-quoted string could have quotes in the
    middle.  Quotes are removed automatically after parsing.

    It basically works like :func:`parse_set_header` just that items
    may appear multiple times and case sensitivity is preserved.

    The return value is a standard :class:`list`:

    &gt;&gt;&gt; parse_list_header(&#39;token, &quot;quoted value&quot;&#39;)
    [&#39;token&#39;, &#39;quoted value&#39;]

    To create a header from the :class:`list` again, use the
    :func:`dump_header` function.

    :param value: a string with a list header.
    :return: :class:`list`
    :rtype: list
    &quot;&quot;&quot;
    result = []
    for item in _parse_list_header(value):
        if item[:1] == item[-1:] == &#39;&quot;&#39;:
            item = unquote_header_value(item[1:-1])
        result.append(item)
    return result


# From mitsuhiko/werkzeug (used with permission).
def parse_dict_header(value):
    &quot;&quot;&quot;Parse lists of key, value pairs as described by RFC 2068 Section 2 and
    convert them into a python dict:

    &gt;&gt;&gt; d = parse_dict_header(&#39;foo=&quot;is a fish&quot;, bar=&quot;as well&quot;&#39;)
    &gt;&gt;&gt; type(d) is dict
    True
    &gt;&gt;&gt; sorted(d.items())
    [(&#39;bar&#39;, &#39;as well&#39;), (&#39;foo&#39;, &#39;is a fish&#39;)]

    If there is no value for a key it will be `None`:

    &gt;&gt;&gt; parse_dict_header(&#39;key_without_value&#39;)
    {&#39;key_without_value&#39;: None}

    To create a header from the :class:`dict` again, use the
    :func:`dump_header` function.

    :param value: a string with a dict header.
    :return: :class:`dict`
    :rtype: dict
    &quot;&quot;&quot;
    result = {}
    for item in _parse_list_header(value):
        if &quot;=&quot; not in item:
            result[item] = None
            continue
        name, value = item.split(&quot;=&quot;, 1)
        if value[:1] == value[-1:] == &#39;&quot;&#39;:
            value = unquote_header_value(value[1:-1])
        result[name] = value
    return result


# From mitsuhiko/werkzeug (used with permission).
def unquote_header_value(value, is_filename=False):
    r&quot;&quot;&quot;Unquotes a header value.  (Reversal of :func:`quote_header_value`).
    This does not use the real unquoting but what browsers are actually
    using for quoting.

    :param value: the header value to unquote.
    :rtype: str
    &quot;&quot;&quot;
    if value and value[0] == value[-1] == &#39;&quot;&#39;:
        # this is not the real unquoting, but fixing this so that the
        # RFC is met will result in bugs with internet explorer and
        # probably some other browsers as well.  IE for example is
        # uploading files with &quot;C:\foo\bar.txt&quot; as filename
        value = value[1:-1]

        # if this is a filename and the starting characters look like
        # a UNC path, then just return the value without quotes.  Using the
        # replace sequence below on a UNC path has the effect of turning
        # the leading double slash into a single slash and then
        # _fix_ie_filename() doesn&#39;t work correctly.  See #458.
        if not is_filename or value[:2] != &quot;\\\\&quot;:
            return value.replace(&quot;\\\\&quot;, &quot;\\&quot;).replace(&#39;\\&quot;&#39;, &#39;&quot;&#39;)
    return value


def dict_from_cookiejar(cj):
    &quot;&quot;&quot;Returns a key/value dictionary from a CookieJar.

    :param cj: CookieJar object to extract cookies from.
    :rtype: dict
    &quot;&quot;&quot;

    cookie_dict = {cookie.name: cookie.value for cookie in cj}
    return cookie_dict


def add_dict_to_cookiejar(cj, cookie_dict):
    &quot;&quot;&quot;Returns a CookieJar from a key/value dictionary.

    :param cj: CookieJar to insert cookies into.
    :param cookie_dict: Dict of key/values to insert into CookieJar.
    :rtype: CookieJar
    &quot;&quot;&quot;

    return cookiejar_from_dict(cookie_dict, cj)


def get_encodings_from_content(content):
    &quot;&quot;&quot;Returns encodings from given content string.

    :param content: bytestring to extract encodings from.
    &quot;&quot;&quot;
    warnings.warn(
        (
            &quot;In requests 3.0, get_encodings_from_content will be removed. For &quot;
            &quot;more information, please see the discussion on issue #2266. (This&quot;
            &quot; warning should only appear once.)&quot;
        ),
        DeprecationWarning,
    )

    charset_re = re.compile(r&#39;&lt;meta.*?charset=[&quot;\&#39;]*(.+?)[&quot;\&#39;&gt;]&#39;, flags=re.I)
    pragma_re = re.compile(r&#39;&lt;meta.*?content=[&quot;\&#39;]*;?charset=(.+?)[&quot;\&#39;&gt;]&#39;, flags=re.I)
    xml_re = re.compile(r&#39;^&lt;\?xml.*?encoding=[&quot;\&#39;]*(.+?)[&quot;\&#39;&gt;]&#39;)

    return (
        charset_re.findall(content)
        + pragma_re.findall(content)
        + xml_re.findall(content)
    )


def _parse_content_type_header(header):
    &quot;&quot;&quot;Returns content type and parameters from given header

    :param header: string
    :return: tuple containing content type and dictionary of
         parameters
    &quot;&quot;&quot;

    tokens = header.split(&quot;;&quot;)
    content_type, params = tokens[0].strip(), tokens[1:]
    params_dict = {}
    items_to_strip = &quot;\&quot;&#39; &quot;

    for param in params:
        param = param.strip()
        if param:
            key, value = param, True
            index_of_equals = param.find(&quot;=&quot;)
            if index_of_equals != -1:
                key = param[:index_of_equals].strip(items_to_strip)
                value = param[index_of_equals + 1 :].strip(items_to_strip)
            params_dict[key.lower()] = value
    return content_type, params_dict


def get_encoding_from_headers(headers):
    &quot;&quot;&quot;Returns encodings from given HTTP Header Dict.

    :param headers: dictionary to extract encoding from.
    :rtype: str
    &quot;&quot;&quot;

    content_type = headers.get(&quot;content-type&quot;)

    if not content_type:
        return None

    content_type, params = _parse_content_type_header(content_type)

    if &quot;charset&quot; in params:
        return params[&quot;charset&quot;].strip(&quot;&#39;\&quot;&quot;)

    if &quot;text&quot; in content_type:
        return &quot;ISO-8859-1&quot;

    if &quot;application/json&quot; in content_type:
        # Assume UTF-8 based on RFC 4627: https://www.ietf.org/rfc/rfc4627.txt since the charset was unset
        return &quot;utf-8&quot;


def stream_decode_response_unicode(iterator, r):
    &quot;&quot;&quot;Stream decodes an iterator.&quot;&quot;&quot;

    if r.encoding is None:
        yield from iterator
        return

    decoder = codecs.getincrementaldecoder(r.encoding)(errors=&quot;replace&quot;)
    for chunk in iterator:
        rv = decoder.decode(chunk)
        if rv:
            yield rv
    rv = decoder.decode(b&quot;&quot;, final=True)
    if rv:
        yield rv


def iter_slices(string, slice_length):
    &quot;&quot;&quot;Iterate over slices of a string.&quot;&quot;&quot;
    pos = 0
    if slice_length is None or slice_length &lt;= 0:
        slice_length = len(string)
    while pos &lt; len(string):
        yield string[pos : pos + slice_length]
        pos += slice_length


def get_unicode_from_response(r):
    &quot;&quot;&quot;Returns the requested content back in unicode.

    :param r: Response object to get unicode content from.

    Tried:

    1. charset from content-type
    2. fall back and replace all unicode characters

    :rtype: str
    &quot;&quot;&quot;
    warnings.warn(
        (
            &quot;In requests 3.0, get_unicode_from_response will be removed. For &quot;
            &quot;more information, please see the discussion on issue #2266. (This&quot;
            &quot; warning should only appear once.)&quot;
        ),
        DeprecationWarning,
    )

    tried_encodings = []

    # Try charset from content-type
    encoding = get_encoding_from_headers(r.headers)

    if encoding:
        try:
            return str(r.content, encoding)
        except UnicodeError:
            tried_encodings.append(encoding)

    # Fall back:
    try:
        return str(r.content, encoding, errors=&quot;replace&quot;)
    except TypeError:
        return r.content


# The unreserved URI characters (RFC 3986)
UNRESERVED_SET = frozenset(
    &quot;ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz&quot; + &quot;0123456789-._~&quot;
)


def unquote_unreserved(uri):
    &quot;&quot;&quot;Un-escape any percent-escape sequences in a URI that are unreserved
    characters. This leaves all reserved, illegal and non-ASCII bytes encoded.

    :rtype: str
    &quot;&quot;&quot;
    parts = uri.split(&quot;%&quot;)
    for i in range(1, len(parts)):
        h = parts[i][0:2]
        if len(h) == 2 and h.isalnum():
            try:
                c = chr(int(h, 16))
            except ValueError:
                raise InvalidURL(f&quot;Invalid percent-escape sequence: &#39;{h}&#39;&quot;)

            if c in UNRESERVED_SET:
                parts[i] = c + parts[i][2:]
            else:
                parts[i] = f&quot;%{parts[i]}&quot;
        else:
            parts[i] = f&quot;%{parts[i]}&quot;
    return &quot;&quot;.join(parts)


def requote_uri(uri):
    &quot;&quot;&quot;Re-quote the given URI.

    This function passes the given URI through an unquote/quote cycle to
    ensure that it is fully and consistently quoted.

    :rtype: str
    &quot;&quot;&quot;
    safe_with_percent = &quot;!#$%&amp;&#39;()*+,/:;=?@[]~&quot;
    safe_without_percent = &quot;!#$&amp;&#39;()*+,/:;=?@[]~&quot;
    try:
        # Unquote only the unreserved characters
        # Then quote only illegal characters (do not quote reserved,
        # unreserved, or &#39;%&#39;)
        return quote(unquote_unreserved(uri), safe=safe_with_percent)
    except InvalidURL:
        # We couldn&#39;t unquote the given URI, so let&#39;s try quoting it, but
        # there may be unquoted &#39;%&#39;s in the URI. We need to make sure they&#39;re
        # properly quoted so they do not cause issues elsewhere.
        return quote(uri, safe=safe_without_percent)


def address_in_network(ip, net):
    &quot;&quot;&quot;This function allows you to check if an IP belongs to a network subnet

    Example: returns True if ip = 192.168.1.1 and net = 192.168.1.0/24
             returns False if ip = 192.168.1.1 and net = 192.168.100.0/24

    :rtype: bool
    &quot;&quot;&quot;
    ipaddr = struct.unpack(&quot;=L&quot;, socket.inet_aton(ip))[0]
    netaddr, bits = net.split(&quot;/&quot;)
    netmask = struct.unpack(&quot;=L&quot;, socket.inet_aton(dotted_netmask(int(bits))))[0]
    network = struct.unpack(&quot;=L&quot;, socket.inet_aton(netaddr))[0] &amp; netmask
    return (ipaddr &amp; netmask) == (network &amp; netmask)


def dotted_netmask(mask):
    &quot;&quot;&quot;Converts mask from /xx format to xxx.xxx.xxx.xxx

    Example: if mask is 24 function returns 255.255.255.0

    :rtype: str
    &quot;&quot;&quot;
    bits = 0xFFFFFFFF ^ (1 &lt;&lt; 32 - mask) - 1
    return socket.inet_ntoa(struct.pack(&quot;&gt;I&quot;, bits))


def is_ipv4_address(string_ip):
    &quot;&quot;&quot;
    :rtype: bool
    &quot;&quot;&quot;
    try:
        socket.inet_aton(string_ip)
    except OSError:
        return False
    return True


def is_valid_cidr(string_network):
    &quot;&quot;&quot;
    Very simple check of the cidr format in no_proxy variable.

    :rtype: bool
    &quot;&quot;&quot;
    if string_network.count(&quot;/&quot;) == 1:
        try:
            mask = int(string_network.split(&quot;/&quot;)[1])
        except ValueError:
            return False

        if mask &lt; 1 or mask &gt; 32:
            return False

        try:
            socket.inet_aton(string_network.split(&quot;/&quot;)[0])
        except OSError:
            return False
    else:
        return False
    return True


@contextlib.contextmanager
def set_environ(env_name, value):
    &quot;&quot;&quot;Set the environment variable &#39;env_name&#39; to &#39;value&#39;

    Save previous value, yield, and then restore the previous value stored in
    the environment variable &#39;env_name&#39;.

    If &#39;value&#39; is None, do nothing&quot;&quot;&quot;
    value_changed = value is not None
    if value_changed:
        old_value = os.environ.get(env_name)
        os.environ[env_name] = value
    try:
        yield
    finally:
        if value_changed:
            if old_value is None:
                del os.environ[env_name]
            else:
                os.environ[env_name] = old_value


def should_bypass_proxies(url, no_proxy):
    &quot;&quot;&quot;
    Returns whether we should bypass proxies or not.

    :rtype: bool
    &quot;&quot;&quot;

    # Prioritize lowercase environment variables over uppercase
    # to keep a consistent behaviour with other http projects (curl, wget).
    def get_proxy(key):
        return os.environ.get(key) or os.environ.get(key.upper())

    # First check whether no_proxy is defined. If it is, check that the URL
    # we&#39;re getting isn&#39;t in the no_proxy list.
    no_proxy_arg = no_proxy
    if no_proxy is None:
        no_proxy = get_proxy(&quot;no_proxy&quot;)
    parsed = urlparse(url)

    if parsed.hostname is None:
        # URLs don&#39;t always have hostnames, e.g. file:/// urls.
        return True

    if no_proxy:
        # We need to check whether we match here. We need to see if we match
        # the end of the hostname, both with and without the port.
        no_proxy = (host for host in no_proxy.replace(&quot; &quot;, &quot;&quot;).split(&quot;,&quot;) if host)

        if is_ipv4_address(parsed.hostname):
            for proxy_ip in no_proxy:
                if is_valid_cidr(proxy_ip):
                    if address_in_network(parsed.hostname, proxy_ip):
                        return True
                elif parsed.hostname == proxy_ip:
                    # If no_proxy ip was defined in plain IP notation instead of cidr notation &amp;
                    # matches the IP of the index
                    return True
        else:
            host_with_port = parsed.hostname
            if parsed.port:
                host_with_port += f&quot;:{parsed.port}&quot;

            for host in no_proxy:
                if parsed.hostname.endswith(host) or host_with_port.endswith(host):
                    # The URL does match something in no_proxy, so we don&#39;t want
                    # to apply the proxies on this URL.
                    return True

    with set_environ(&quot;no_proxy&quot;, no_proxy_arg):
        # parsed.hostname can be `None` in cases such as a file URI.
        try:
            bypass = proxy_bypass(parsed.hostname)
        except (TypeError, socket.gaierror):
            bypass = False

    if bypass:
        return True

    return False


def get_environ_proxies(url, no_proxy=None):
    &quot;&quot;&quot;
    Return a dict of environment proxies.

    :rtype: dict
    &quot;&quot;&quot;
    if should_bypass_proxies(url, no_proxy=no_proxy):
        return {}
    else:
        return getproxies()


def select_proxy(url, proxies):
    &quot;&quot;&quot;Select a proxy for the url, if applicable.

    :param url: The url being for the request
    :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs
    &quot;&quot;&quot;
    proxies = proxies or {}
    urlparts = urlparse(url)
    if urlparts.hostname is None:
        return proxies.get(urlparts.scheme, proxies.get(&quot;all&quot;))

    proxy_keys = [
        urlparts.scheme + &quot;://&quot; + urlparts.hostname,
        urlparts.scheme,
        &quot;all://&quot; + urlparts.hostname,
        &quot;all&quot;,
    ]
    proxy = None
    for proxy_key in proxy_keys:
        if proxy_key in proxies:
            proxy = proxies[proxy_key]
            break

    return proxy


def resolve_proxies(request, proxies, trust_env=True):
    &quot;&quot;&quot;This method takes proxy information from a request and configuration
    input to resolve a mapping of target proxies. This will consider settings
    such as NO_PROXY to strip proxy configurations.

    :param request: Request or PreparedRequest
    :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs
    :param trust_env: Boolean declaring whether to trust environment configs

    :rtype: dict
    &quot;&quot;&quot;
    proxies = proxies if proxies is not None else {}
    url = request.url
    scheme = urlparse(url).scheme
    no_proxy = proxies.get(&quot;no_proxy&quot;)
    new_proxies = proxies.copy()

    if trust_env and not should_bypass_proxies(url, no_proxy=no_proxy):
        environ_proxies = get_environ_proxies(url, no_proxy=no_proxy)

        proxy = environ_proxies.get(scheme, environ_proxies.get(&quot;all&quot;))

        if proxy:
            new_proxies.setdefault(scheme, proxy)
    return new_proxies


def default_user_agent(name=&quot;python-requests&quot;):
    &quot;&quot;&quot;
    Return a string representing the default user agent.

    :rtype: str
    &quot;&quot;&quot;
    return f&quot;{name}/{__version__}&quot;


def default_headers():
    &quot;&quot;&quot;
    :rtype: requests.structures.CaseInsensitiveDict
    &quot;&quot;&quot;
    return CaseInsensitiveDict(
        {
            &quot;User-Agent&quot;: default_user_agent(),
            &quot;Accept-Encoding&quot;: DEFAULT_ACCEPT_ENCODING,
            &quot;Accept&quot;: &quot;*/*&quot;,
            &quot;Connection&quot;: &quot;keep-alive&quot;,
        }
    )


def parse_header_links(value):
    &quot;&quot;&quot;Return a list of parsed link headers proxies.

    i.e. Link: &lt;http:/.../front.jpeg&gt;; rel=front; type=&quot;image/jpeg&quot;,&lt;http://.../back.jpeg&gt;; rel=back;type=&quot;image/jpeg&quot;

    :rtype: list
    &quot;&quot;&quot;

    links = []

    replace_chars = &quot; &#39;\&quot;&quot;

    value = value.strip(replace_chars)
    if not value:
        return links

    for val in re.split(&quot;, *&lt;&quot;, value):
        try:
            url, params = val.split(&quot;;&quot;, 1)
        except ValueError:
            url, params = val, &quot;&quot;

        link = {&quot;url&quot;: url.strip(&quot;&lt;&gt; &#39;\&quot;&quot;)}

        for param in params.split(&quot;;&quot;):
            try:
                key, value = param.split(&quot;=&quot;)
            except ValueError:
                break

            link[key.strip(replace_chars)] = value.strip(replace_chars)

        links.append(link)

    return links


# Null bytes; no need to recreate these on each call to guess_json_utf
_null = &quot;\x00&quot;.encode(&quot;ascii&quot;)  # encoding to ASCII for Python 3
_null2 = _null * 2
_null3 = _null * 3


def guess_json_utf(data):
    &quot;&quot;&quot;
    :rtype: str
    &quot;&quot;&quot;
    # JSON always starts with two ASCII characters, so detection is as
    # easy as counting the nulls and from their location and count
    # determine the encoding. Also detect a BOM, if present.
    sample = data[:4]
    if sample in (codecs.BOM_UTF32_LE, codecs.BOM_UTF32_BE):
        return &quot;utf-32&quot;  # BOM included
    if sample[:3] == codecs.BOM_UTF8:
        return &quot;utf-8-sig&quot;  # BOM included, MS style (discouraged)
    if sample[:2] in (codecs.BOM_UTF16_LE, codecs.BOM_UTF16_BE):
        return &quot;utf-16&quot;  # BOM included
    nullcount = sample.count(_null)
    if nullcount == 0:
        return &quot;utf-8&quot;
    if nullcount == 2:
        if sample[::2] == _null2:  # 1st and 3rd are null
            return &quot;utf-16-be&quot;
        if sample[1::2] == _null2:  # 2nd and 4th are null
            return &quot;utf-16-le&quot;
        # Did not detect 2 valid UTF-16 ascii-range characters
    if nullcount == 3:
        if sample[:3] == _null3:
            return &quot;utf-32-be&quot;
        if sample[1:] == _null3:
            return &quot;utf-32-le&quot;
        # Did not detect a valid UTF-32 ascii-range character
    return None


def prepend_scheme_if_needed(url, new_scheme):
    &quot;&quot;&quot;Given a URL that may or may not have a scheme, prepend the given scheme.
    Does not replace a present scheme with the one provided as an argument.

    :rtype: str
    &quot;&quot;&quot;
    parsed = parse_url(url)
    scheme, auth, host, port, path, query, fragment = parsed

    # A defect in urlparse determines that there isn&#39;t a netloc present in some
    # urls. We previously assumed parsing was overly cautious, and swapped the
    # netloc and path. Due to a lack of tests on the original defect, this is
    # maintained with parse_url for backwards compatibility.
    netloc = parsed.netloc
    if not netloc:
        netloc, path = path, netloc

    if auth:
        # parse_url doesn&#39;t provide the netloc with auth
        # so we&#39;ll add it ourselves.
        netloc = &quot;@&quot;.join([auth, netloc])
    if scheme is None:
        scheme = new_scheme
    if path is None:
        path = &quot;&quot;

    return urlunparse((scheme, netloc, path, &quot;&quot;, query, fragment))


def get_auth_from_url(url):
    &quot;&quot;&quot;Given a url with authentication components, extract them into a tuple of
    username,password.

    :rtype: (str,str)
    &quot;&quot;&quot;
    parsed = urlparse(url)

    try:
        auth = (unquote(parsed.username), unquote(parsed.password))
    except (AttributeError, TypeError):
        auth = (&quot;&quot;, &quot;&quot;)

    return auth


def check_header_validity(header):
    &quot;&quot;&quot;Verifies that header parts don&#39;t contain leading whitespace
    reserved characters, or return characters.

    :param header: tuple, in the format (name, value).
    &quot;&quot;&quot;
    name, value = header
    _validate_header_part(header, name, 0)
    _validate_header_part(header, value, 1)


def _validate_header_part(header, header_part, header_validator_index):
    if isinstance(header_part, str):
        validator = _HEADER_VALIDATORS_STR[header_validator_index]
    elif isinstance(header_part, bytes):
        validator = _HEADER_VALIDATORS_BYTE[header_validator_index]
    else:
        raise InvalidHeader(
            f&quot;Header part ({header_part!r}) from {header} &quot;
            f&quot;must be of type str or bytes, not {type(header_part)}&quot;
        )

    if not validator.match(header_part):
        header_kind = &quot;name&quot; if header_validator_index == 0 else &quot;value&quot;
        raise InvalidHeader(
            f&quot;Invalid leading whitespace, reserved character(s), or return &quot;
            f&quot;character(s) in header {header_kind}: {header_part!r}&quot;
        )


def urldefragauth(url):
    &quot;&quot;&quot;
    Given a url remove the fragment and the authentication part.

    :rtype: str
    &quot;&quot;&quot;
    scheme, netloc, path, params, query, fragment = urlparse(url)

    # see func:`prepend_scheme_if_needed`
    if not netloc:
        netloc, path = path, netloc

    netloc = netloc.rsplit(&quot;@&quot;, 1)[-1]

    return urlunparse((scheme, netloc, path, params, query, &quot;&quot;))


def rewind_body(prepared_request):
    &quot;&quot;&quot;Move file pointer back to its recorded starting position
    so it can be read again on redirect.
    &quot;&quot;&quot;
    body_seek = getattr(prepared_request.body, &quot;seek&quot;, None)
    if body_seek is not None and isinstance(
        prepared_request._body_position, integer_types
    ):
        try:
            body_seek(prepared_request._body_position)
        except OSError:
            raise UnrewindableBodyError(
                &quot;An error occurred when rewinding request body for redirect.&quot;
            )
    else:
        raise UnrewindableBodyError(&quot;Unable to rewind request body for redirect.&quot;)
</pre></div>
</div>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Adam Worsnip.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>